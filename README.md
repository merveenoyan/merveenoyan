<img width="966" alt="Banner" src="https://i.pinimg.com/originals/d2/e5/53/d2e553974136d4c684d55744478bbf36.jpg">


<h1 align="center">Hi ğŸ‘‹, I'm Merve</h1>

I build, write, showcase around zero-shot vision, multimodality, optimization and more at Hugging Face (mostly transformers). Check out my pinned projects on GH.
My [Hugging Face profile](https://huggingface.co/merve) has a lot of cool stuff and I also write blogs over there. 

ğŸŒ± [smol-vision: notebooks, scripts and more on various zero-shot vision/multimodal model optimizations](https://github.com/merveenoyan/smol-vision)

ğŸ”– [Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models](https://huggingface.co/blog/finetune-florence2)

ğŸ”– [Vision Language Models Explained](https://huggingface.co/blog/vlms)

ğŸ”– [PaliGemma â€“ Google's Cutting-Edge Open Vision Language Model](https://huggingface.co/blog/paligemma)

ğŸ”– [Introduction to Quantization](https://huggingface.co/blog/merve/quantization)

â–¶ï¸ [A walkthrough on multimodality, papers, tools and more](https://www.youtube.com/watch?v=IoGaGfU1CIg)

â–¶ï¸ [A video on open-source LLMs, where to find them, how to eval and deploy](https://www.youtube.com/watch?v=e9gNEAlsOvU)

â–¶ï¸ [A walkthrough on zero-shot vision, papers, tools and more](https://www.youtube.com/watch?v=BnM-S50P_so)


## ğŸ”— Let's Connect!
<a href="https://twitter.com/mervenoyann" target="_blank"><img alt="Twitter" src="https://img.shields.io/badge/twitter-%231DA1F2.svg?&style=for-the-badge&logo=twitter&logoColor=white" /></a>
<a href="https://medium.com/@merveenoyan" target="_blank"><img alt="Medium" src="https://img.shields.io/badge/medium-%2312100E.svg?&style=for-the-badge&logo=medium&logoColor=white" /></a>
<a href="https://www.linkedin.com/in/merve-noyan-28b1a113a/" target="_blank"><img alt="LinkedIn" src="https://img.shields.io/badge/linkedin-%230077B5.svg?&style=for-the-badge&logo=linkedin&logoColor=white" /></a>
