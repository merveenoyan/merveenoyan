<img width="966" alt="Banner" src="https://github.com/merveenoyan/merveenoyan/assets/53175384/57bd2415-9b8b-49a9-8bba-87696a4c27fc">


<h1 align="center">Hi ğŸ‘‹, I'm Merve</h1>

I build, write, showcase around zero-shot vision, multimodality, optimization and more at Hugging Face (mostly transformers). Check out my pinned projects on GH.
My [Hugging Face profile](https://huggingface.co/merve) has a lot of cool stuff and I also write blogs over there. 

â–¶ï¸ [A walkthrough on multimodality, papers, tools and more](https://www.youtube.com/watch?v=IoGaGfU1CIg)
â–¶ï¸ [A video on open-source LLMs, where to find them, how to eval and deploy](https://www.youtube.com/watch?v=e9gNEAlsOvU)
â–¶ï¸ [A walkthrough on zero-shot vision, papers, tools and more](https://www.youtube.com/watch?v=BnM-S50P_so)

ğŸ”– [Fine-tuning Florence-2 - Microsoft's Cutting-edge Vision Language Models](https://huggingface.co/blog/finetune-florence2)
ğŸ”– [Vision Language Models Explained](https://huggingface.co/blog/vlms)
ğŸ”– [PaliGemma â€“ Google's Cutting-Edge Open Vision Language Model](https://huggingface.co/blog/paligemma)
ğŸ”– [Introduction to Quantization](https://huggingface.co/blog/merve/quantization)


## ğŸ”— Let's Connect!
<a href="https://twitter.com/mervenoyann" target="_blank"><img alt="Twitter" src="https://img.shields.io/badge/twitter-%231DA1F2.svg?&style=for-the-badge&logo=twitter&logoColor=white" /></a>
<a href="https://medium.com/@merveenoyan" target="_blank"><img alt="Medium" src="https://img.shields.io/badge/medium-%2312100E.svg?&style=for-the-badge&logo=medium&logoColor=white" /></a>
<a href="https://www.linkedin.com/in/merve-noyan-28b1a113a/" target="_blank"><img alt="LinkedIn" src="https://img.shields.io/badge/linkedin-%230077B5.svg?&style=for-the-badge&logo=linkedin&logoColor=white" /></a>
